{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=6>Unidad 5. Fase de modelado</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Italo Contreras Peerez</font><br>\n",
    "<font color=\"#004D7F\" size=3>Machine Learning con Python</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "    * [1.1. Cargando mi dataset](#section11)\n",
    "* [2. Algoritmos de Regresión](#section2)\n",
    "    * [2.1. Algoritmos de taxonomía lineal](#section21)\n",
    "    * [2.2. Algoritmos de taxonomía no lineal](#section22)\n",
    "    * [2.3. Plantilla para comparar algoritmos](#section23)    \n",
    "* [3. Algoritmos de conjunto](#section3)\n",
    "    * [3.1. Algoritmo Bagging](#section31)\n",
    "    * [3.1. Algoritmo Boosting](#section32)\n",
    "    * [3.1. Algoritmo Voting](#section33)    \n",
    "* [4. Super Lerner](#section4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{ width:98% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite ajustar la anchura de la parte útil de la libreta (reduce los márgenes)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container{ width:98% }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, vamos a analizar algunos de estos algoritmos, en especial, los de taxonomía lineal y no lineal. En cuanto a la taxonomía de conjunto o ensamblados, como los tipo boosting y bagging, los veremos posteriormente cuando ya tengamos una base sólida de estos primeros. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section12\"></a>\n",
    "## <font color=\"#004D7F\"> 1.1. Cargando mi dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a seguir usando el dataset que escogi desde un prinicipio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "filename_reg = 'data/forestfires.csv'\n",
    "data = pd.read_csv(filename_reg)\n",
    "data1=data.drop(['month'], axis=1)\n",
    "data2=data1.drop(['day'], axis=1)\n",
    "#names=['X','Y','FFMC','DMC','DC','ISI','temp','RH','wind','rain','area']\n",
    "array_reg = data2.values\n",
    "X=data2.drop(\"area\",1)\n",
    "Y=data2[\"area\"]\n",
    "#Dividamos la data de esta forma par el SuperLerner\n",
    "X_SL=array_reg[:,0:10]\n",
    "Y_SL=array_reg[:,10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\"> 2. Algoritmos de Regresión</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se estudiarán seis algoritmos de Machine Learning que puede usar al verificar su problema de regresión en Python con scikit-learn. Para ello, vamos a echar un vistazo a siete algoritmos de regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "## <font color=\"#004D7F\"> 2.1. Algoritmos de taxonomía lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con cuatro algoritmos lineales:\n",
    "* Linear Regression (LiR).\n",
    "* Ridge Regression (RiR).\n",
    "* LASSO Linear Regression (LASSO). \n",
    "* Elastic Net Regression (ENR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section211\"></a>\n",
    "### <font color=\"#004D7F\"> 2.1.1. Linear Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiR supone que las variables de entrada tienen una distribución gaussiana. También se supone que las variables de entrada son relevantes para la variable de salida y que no están altamente correlacionadas entre sí (un problema llamado colinealidad). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4128.861078750497\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=LinearRegression()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que obtenemos un error muy alto, vamos a tratar de reducir esto usando otros modelos de taxonomia lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section212\"></a>\n",
    "### <font color=\"#004D7F\"> 2.1.2. Ridge Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RiR es una extensión de LiR donde la función de pérdida se modifica para minimizar la complejidad del modelo medido como el valor de la suma cuadrática de los valores del coeficiente (también llamada norma __L2__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4118.679963611857\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=Ridge()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con este modelo obtenemos un error menor que con el de regresion lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section213\"></a>\n",
    "### <font color=\"#004D7F\"> 2.1.3. LASSO Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO es una modificación de LiR, como RiR, donde la función de pérdida se modifica para minimizar la complejidad del modelo medido como el valor absoluto de los valores del coeficiente (también llamada la norma __L1__). Puede construir un modelo LASSO utilizando la clase `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4110.55839395247\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=Lasso()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con el modelo LASSO obtenemos un error menor que los 2 anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section214\"></a>\n",
    "### <font color=\"#004D7F\"> 2.1.4. ElasticNet Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENR es una forma de regresión de regularización que combina las propiedades de RIR y LASSO. Busca minimizar la complejidad del modelo de regresión (magnitud y número de coeficientes de regresión) penalizando el modelo utilizando tanto la norma L2 (valores de coeficiente de suma cuadrática) como la norma L1 (valores de coeficiente absoluto de suma). Puede construir un modelo ENR utilizando la clase `ElasticNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4109.062032102183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "# Linear Regression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=ElasticNet()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con este modelo de todos los de taxonomia lineal que estamos viendo se tiene un menor error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section22\"></a>\n",
    "## <font color=\"#004D7F\"> 2.2. Algoritmos de taxonomía no lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, estudiaremos tres algoritmos no lineales:\n",
    "* $k$-Nearest Neighbors ($k$-NN).\n",
    "* Classification and Regression Trees (CART). \n",
    "* Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section221\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.1. $k$-Nearest Neighbours</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-NN localiza las $k$ instancias más similares en el conjunto de datos de entrenamiento para una nueva instancia de datos. De los $k$ vecinos, se toma una variable de salida media o mediana como la predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -5331.62783328371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# k-NN Regressionore\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=KNeighborsRegressor()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el error se ha disparado, en comparacion a los 4 modelos de arriba, lo bueno es que este modelo tienes varios hiperparametros que se les puede modificar de tal forma que este error se reduzca lo mayor posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Por ejemplo ahora vamos ahora a modificar los hiperparametros, por ejemplo cambiaremos el algorithm,los vecinos y el weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4671.965507925192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# k-NN Regressionore\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=KNeighborsRegressor(n_neighbors=9,algorithm='kd_tree',weights='distance')\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nuestro error ha disminuido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section222\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.2. Classification and Regression Trees</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART usa los datos de entrenamiento para seleccionar los mejores puntos para dividir los datos con el fin de minimizar una métrica de costos. La métrica de costo predeterminada para los árboles de decisión de regresión es el error cuadrático medio, especificado en el parámetro `criterion`. Puede construir un modelo CART utilizando la clase `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -11795.850850700415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=DecisionTreeRegressor(criterion='friedman_mse')\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section223\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.3. Support Vector Machines</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM se desarrollaró para la clasificación binaria. La técnica se ha extendido para los problemas de predicción de valores reales llamados Support Vector Regression (SVR). Primero vamos a usar un modelo sin ponerle parametros(en este caso toma por defecto al parametro kernel con un valor de 'rbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4202.62502384061\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=SVR()\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nos sale un error alto. Ahora vamos a modificarle 2 parametros, el kernel y el degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4185.091464706861\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=SVR(kernel='sigmoid',degree=3)\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el error se ha reducido un poco. Ahora vamos a darle al parametro kernel el valor 'linear', con esto vamos a hacer que nuestro svm se comporto como un modelo de taxonomia lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -4193.396414864668\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "kfold=KFold(n_splits=10,random_state=7)\n",
    "model=SVR(kernel='linear')\n",
    "scoring='neg_mean_squared_error'\n",
    "results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "print(f\"MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún asi vemos que nuestro modelo tiene mucho error, esto puede ser porque nuestros datos no se ajustan a un modelo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section23\"></a>\n",
    "## <font color=\"#004D7F\"> 2.3. Plantilla para comparar algoritmos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiR: -4128.861078750497 (7977.634994360253%)\n",
      "Ridge: -4118.679963611857 (7980.55101357425%)\n",
      "Lasso: -4110.55839395247 (7992.040254499503%)\n",
      "ElN: -4109.062032102183 (7992.440799899827%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN: -5331.62783328371 (7966.558312232972%)\n",
      "CART: -11770.08231522153 (20041.88459979405%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: -4193.396414864668 (8343.068466551533%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib import pyplot\n",
    "\n",
    "models= []\n",
    "\n",
    "models.append(( 'LiR',LinearRegression( )))\n",
    "models.append(( 'Ridge',Ridge()))\n",
    "models.append(( 'Lasso',Lasso()))\n",
    "models.append(( 'ElN',ElasticNet()))\n",
    "models.append(( 'KNN',KNeighborsRegressor()))\n",
    "models.append(( 'CART' ,DecisionTreeRegressor(criterion='friedman_mse')))\n",
    "models.append(( 'SVR' , SVR(kernel='linear')))\n",
    "\n",
    "results=[]           \n",
    "names= []\n",
    "scoring='neg_mean_squared_error'\n",
    "\n",
    "for name, model in models: \n",
    "    kfold = KFold( n_splits=10, random_state=7) \n",
    "    cv_results = cross_val_score (model, X, Y, cv=kfold , scoring=scoring) \n",
    "    results.append(cv_results) \n",
    "    names.append(name) \n",
    "    print(f\"{name}: {cv_results.mean()} ({ cv_results.std()}%)\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot algorithm comparison \n",
    "fig = pyplot.figure() \n",
    "fig.suptitle('Comparacio de Algoritmos') \n",
    "ax = fig.add_subplot(111) \n",
    "pyplot.boxplot( results) \n",
    "ax.set_xticklabels(names) \n",
    "pyplot .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el ElN es el modelo que tiene un error menor y tambien una menor varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\"> 3. Algoritmos de conjunto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando usamos distintos estimadores para un mismo conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "## <font color=\"#004D7F\"> 3.1. Algoritmos Bagging</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section311\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.1. Bagged Decision Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -109.74255583209103(322.76624492662876%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import BaggingRegressor\n",
    "from sklearn . tree import DecisionTreeRegressor \n",
    "seed = 7 \n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "cart = DecisionTreeRegressor () \n",
    "num_trees = 100 \n",
    "model = BaggingRegressor( base_estimator=cart , n_estimators=num_trees , random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section312\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.2. Random Forest</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -85.82939342796419(250.2076745810255%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import RandomForestRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100 \n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = RandomForestRegressor( n_estimators=num_trees ,max_features=max_features, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section313\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.3. Extra Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -173.21683635450495(500.2481313017765%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import ExtraTreesRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100 \n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = ExtraTreesRegressor( n_estimators=num_trees ,max_features=max_features, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "## <font color=\"#004D7F\"> 3.2. Algoritmos Boosting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien llamados algorimtos inteligentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section321\"></a>\n",
    "### <font color=\"#004D7F\"> 3.2.1. AdaBoost</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -169.13316518650453(494.25621594157536%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import AdaBoostRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = AdaBoostRegressor( n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -294.51571159082715(854.2300759223095%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import AdaBoostRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = AdaBoostRegressor( n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que al aumentar la cantidad de arboles tenemos peores resultados ya que el error se incrementa y la varianza también"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section322\"></a>\n",
    "### <font color=\"#004D7F\"> 3.2.2. Stochastic Gradient Boosting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un algoritmo muy sofisticado, que se dispara bastante en el consumo computancional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -67.56414196486551(181.5561008831879%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import GradientBoostingRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = GradientBoostingRegressor( n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos poco error, pero la varianza sigue siendo alta, ahora vamos a ponerle un valor al hiperparametro learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -115.67702684913942(313.93853304159626%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn .ensemble import GradientBoostingRegressor\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features=3\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=seed) \n",
    "model = GradientBoostingRegressor( n_estimators=num_trees,learning_rate=0.3, random_state=seed) \n",
    "results = cross_val_score (model, X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos un peor modelo, hay mas error y varianza, pero igual debemos probar con distintos hiperparametros porque puede para un conjunto de datos sí se mejore el modelo al ponerle un learning_rate que no sea el que esta por defecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section33\"></a>\n",
    "## <font color=\"#004D7F\"> 3.3. Algoritmo Voting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un algoritmo un poco mas especializado porque usamos distintos estimadores y ver una ponderacion de esos resultados. Este algoritmo puede dar buenos resultados, aunque es un poco complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Italo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -38.586852241008835(109.6856989922095%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import cross_val_score \n",
    "from sklearn . linear_model import LinearRegression \n",
    "from sklearn . tree import DecisionTreeRegressor \n",
    "from sklearn .svm import SVR\n",
    "from sklearn .ensemble import VotingRegressor \n",
    "\n",
    "\n",
    "kfold = KFold( n_splits=10, random_state=7) \n",
    "# creando los submodelos \n",
    "estimators = [] \n",
    "model1 = LinearRegression() \n",
    "estimators.append(('regression', model1) ) \n",
    "model2=DecisionTreeRegressor(criterion='friedman_mse')\n",
    "estimators.append(( 'cart' , model2) ) \n",
    "model3=SVR(kernel='linear') \n",
    "estimators .append(( 'svr' , model3) ) \n",
    "# creando el modelo ensamblado \n",
    "ensemble = VotingRegressor ( estimators ) \n",
    "results = cross_val_score (ensemble , X, Y, cv=kfold) \n",
    "print(f\"MSE: {results.mean()}({ results.std()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\"> 4. Super Lerner</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "from numpy import hstack \n",
    "from numpy import vstack \n",
    "from numpy import asarray \n",
    "from sklearn . model_selection import KFold \n",
    "from sklearn . model_selection import train_test_split \n",
    "from sklearn . metrics import mean_squared_error \n",
    "from sklearn . linear_model import LinearRegression \n",
    "from sklearn . linear_model import ElasticNet \n",
    "from sklearn . neighbors import KNeighborsRegressor \n",
    "from sklearn . tree import DecisionTreeRegressor \n",
    "from sklearn .svm import SVR \n",
    "from sklearn .ensemble import AdaBoostRegressor \n",
    "from sklearn .ensemble import BaggingRegressor \n",
    "from sklearn .ensemble import RandomForestRegressor \n",
    "from sklearn .ensemble import ExtraTreesRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la vidision de la data para el entrenamiento del SuperLerner y luego el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (258, 10) (258,) Test (259, 10) (259,)\n"
     ]
    }
   ],
   "source": [
    "# Division \n",
    "X_SL, X_val , Y_SL, Y_val = train_test_split (X_SL, Y_SL, test_size =0.50) \n",
    "print ( 'Train' , X_SL.shape , Y_SL.shape , 'Test' , X_val .shape , Y_val .shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando la lista de modelos con los escogidos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_models () : \n",
    "    models = list () \n",
    "    models.append(LinearRegression () ) \n",
    "    models.append( Ridge () ) \n",
    "    models.append(SVR(kernel='linear')) \n",
    "    models.append(DecisionTreeRegressor (criterion='friedman_mse') ) \n",
    "    models.append(KNeighborsRegressor () ) \n",
    "    models.append(AdaBoostRegressor () ) \n",
    "    models.append(BaggingRegressor( n_estimators=10)) \n",
    "    models.append(RandomForestRegressor( n_estimators=10)) \n",
    "    models.append(ExtraTreesRegressor ( n_estimators=10)) \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recolectar las predicciones de una validacion cruzada \n",
    "def get_out_of_fold_predictions (X_SL, Y_SL, models) : \n",
    "    meta_X, meta_Y = list(), list() \n",
    "    # Definicion del kfold \n",
    "    kfold = KFold( n_splits=10, shuffle=True) \n",
    "    # Enumerar las particiones \n",
    "    for train_ix , test_ix in kfold.split(X_SL) : \n",
    "        fold_yhats = list () \n",
    "        # Obetenemos los datos \n",
    "        train_X , test_X = X_SL[ train_ix ] , X_SL[ test_ix ] \n",
    "        train_Y , test_Y = Y_SL[ train_ix ] , Y_SL[ test_ix ] \n",
    "        meta_Y. extend( test_Y ) \n",
    "        # Ajuste y predicciones de cada modelo \n",
    "        for model in models: \n",
    "            model.fit(train_X , train_Y ) \n",
    "            yhat = model. predict ( test_X ) \n",
    "            # Almacena las columnas \n",
    "            fold_yhats.append(yhat . reshape(len (yhat) ,1) ) \n",
    "        # Almacena el fold y yhats como columnas \n",
    "        meta_X.append(hstack( fold_yhats ) ) \n",
    "    return vstack (meta_X) , asarray (meta_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a evaluar el modelo. Primero obtenemos los modelos y usa la funcion de hace un momento para obtener el meta_X y el meta_Y para el metaclasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models ()\n",
    "meta_X, meta_Y = get_out_of_fold_predictions (X_SL, Y_SL, models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahora ajustamos todos los modelos al dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_base_models (X_SL, Y_SL, models) :\n",
    "    for model in models: \n",
    "        model. fit (X_SL, Y_SL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hacemos el ajuste del metamodelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_meta_model(X_SL, Y_SL) : \n",
    "    model = LinearRegression () \n",
    "    model. fit (X_SL, Y_SL) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una funcion de evaluacion de modelos bases para ver cuales son los modelos que tienen peor y mejor puntaje par a mi conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate_models(X_SL, Y_SL, models) : \n",
    "        for model in models: \n",
    "            yhat = model. predict (X_SL) \n",
    "            mse = mean_squared_error(Y_SL, yhat) \n",
    "            print ( '% s : RMSE %.3f ' % (model. __class__ .__name__ , sqrt (mse) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer las predicciones de ese modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_learner_predictions (X_SL, models , meta_model) : \n",
    "    meta_X = list () \n",
    "    for model in models: \n",
    "        yhat = model. predict (X_SL) \n",
    "        meta_X.append(yhat . reshape(len (yhat) ,1) ) \n",
    "    meta_X = hstack(meta_X) \n",
    "    # Prediccion \n",
    "    return meta_model. predict (meta_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ajustamos y evaluamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression : RMSE 33.250 \n",
      "Ridge : RMSE 33.094 \n",
      "SVR : RMSE 33.555 \n",
      "DecisionTreeRegressor : RMSE 45.898 \n",
      "KNeighborsRegressor : RMSE 36.077 \n",
      "AdaBoostRegressor : RMSE 34.015 \n",
      "BaggingRegressor : RMSE 39.117 \n",
      "RandomForestRegressor : RMSE 50.314 \n",
      "ExtraTreesRegressor : RMSE 58.357 \n",
      "Super Learner : RMSE 32.122 \n"
     ]
    }
   ],
   "source": [
    "#Ajuste de modelos base\n",
    "fit_base_models (X_SL, Y_SL, models) \n",
    "# Ajuste del meta−modelo \n",
    "meta_model = fit_meta_model(meta_X, meta_Y) \n",
    "# Evaluar modelos base \n",
    "evaluate_models(X_val , Y_val , models) \n",
    "# evaluar el meta−modelo \n",
    "yhat = super_learner_predictions (X_val , models , meta_model) \n",
    "print ( 'Super Learner : RMSE %.3f ' % ( sqrt (mean_squared_error( Y_val , yhat) ) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los algoritmos de taxonomia lineal se ajustan un poco mejor a mi dataset en comparacion a los algoritmos de conjunto.\n",
    "Vemos que falta configurar un poco estos modelos porque hemos puesto en RandomForest, BagginRegressor y ExtraTrees a 10 estimadores(Arboles), quizas si se configura mejor se tienen mejores resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
